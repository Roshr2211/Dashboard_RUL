# # -*- coding: utf-8 -*-
# """Single_Encoder_transfomer.ipynb

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/1c_4m4GCKQD91LJueYhZAE-EUdESxXaHu

# # NASA dataset
# """

# !git clone https://github.com/Roshr2211/Predicting-RUL-for-EV-Battery.git

"""# Importing Packages"""

import os
import numpy as np
import pandas as pd
from scipy.io import loadmat
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Model
from sklearn.metrics import mean_absolute_error, mean_squared_error

def to_padded_numpy(l, shape):
    padded_array = np.zeros(shape)
    padded_array[:len(l)] = l
    return padded_array

def preprocess_data_to_cycles():
    path = "Predicting-RUL-for-EV-Battery/"
    files = [f for f in os.listdir(path) if f.endswith('.mat')]
    dis_mat = [os.path.join(path, f) for f in files]
    battery_grp = {}

    for f in files:
        key = f.split('.')[0]
        battery_grp[key] = key

    bs = [f.split('.')[0] for f in files]

    ds = []
    for f in dis_mat:
        ds.append(loadmat(f))

    types = []
    times = []
    ambient_temperatures = []
    datas = []

    for i in range(len(ds)):
        x = ds[i][bs[i]]["cycle"][0][0][0]
        ambient_temperatures.append(list(map(lambda y: y[0][0], x['ambient_temperature'])))
        types.append(x['type'])
        times.append(x['time'])
        datas.append(x['data'])

    batteries = []
    cycles = []
    for i in range(len(ds)):
        batteries.append(bs[i])
        cycles.append(datas[i].size)

    battery_cycle_df = pd.DataFrame({'Battery': batteries, 'Cycle': cycles}).sort_values('Battery', ascending=True)
    battery_cycle_df.drop_duplicates(inplace=True)

    Cycles = {}
    params = ['Voltage_measured', 'Current_measured', 'Temperature_measured', 'Current_load', 'Voltage_load', 'Time', 'Capacity']

    for i in range(len(bs)):
        Cycles[bs[i]] = {}
        Cycles[bs[i]]['count'] = 0
        for param in params:
            Cycles[bs[i]][param] = []
            for j in range(datas[i].size):
                if types[i][j] == 'discharge':
                    Cycles[bs[i]][param].append(datas[i][j][param][0][0][0])

        cap = []
        amb_temp = []
        for j in range(datas[i].size):
            if types[i][j] == 'discharge':
                cap.append(datas[i][j]['Capacity'][0][0][0])
                amb_temp.append(ambient_temperatures[i][j])

        Cycles[bs[i]]['Capacity'] = np.array(cap)
        Cycles[bs[i]]['ambient_temperatures'] = np.array(amb_temp)

    Cycles = pd.DataFrame(Cycles)

    return Cycles

def get_exp_based_df(exp):
    Cycles = preprocess_data_to_cycles()
    df_all = pd.DataFrame({})
    max_len = 0

    exp_try_out = exp

    for bat in exp_try_out:
        if bat not in Cycles.columns:
            print(f"Battery {bat} not found in Cycles DataFrame")
            continue

        df = pd.DataFrame({})
        cols = ['Voltage_measured', 'Current_measured', 'Temperature_measured', 'Current_load', 'Voltage_load', 'Time', 'Capacity', 'ambient_temperatures']
        for col in cols:
            df[col] = Cycles[bat][col]
        max_l = np.max(df['Time'].apply(lambda x: len(x)).values)
        max_len = max(max_l, max_len)
        df_all = pd.concat([df_all, df], ignore_index=True)

    df = df_all.reset_index(drop=True)

    for i, j in enumerate(df['Capacity']):
        try:
            if len(j):
                df['Capacity'][i] = j[0]
            else:
                df['Capacity'][i] = 0
        except:
            pass

    df_x = df.drop(columns=['Capacity', 'ambient_temperatures']).values
    df_y = df['Capacity'].values

    n, m = df_x.shape[0], df_x.shape[1]
    temp2 = np.zeros((n, m, max_len))
    for i in range(n):
        for j in range(m):
            temp2[i][j] = to_padded_numpy(df_x[i][j], max_len)

    df_x = temp2
    return df_x, df_y

class TransformerBlock(layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):
        super(TransformerBlock, self).__init__()
        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = tf.keras.Sequential(
            [layers.Dense(ff_dim, activation="relu"), layers.Dense(embed_dim)]
        )
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(rate)
        self.dropout2 = layers.Dropout(rate)

    def call(self, inputs, training):
        attn_output = self.att(inputs, inputs)  # Self-attention
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)

def build_transformer_model(input_shape, embed_dim, num_heads, ff_dim):
    inputs = layers.Input(shape=input_shape)
    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)
    x = layers.Dense(embed_dim)(inputs)
    x = transformer_block(x)
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dropout(0.1)(x)
    x = layers.Dense(20, activation="relu")(x)
    x = layers.Dropout(0.1)(x)
    outputs = layers.Dense(1, activation="linear")(x)
    model = Model(inputs=inputs, outputs=outputs)
    return model

if __name__ == "__main__":
    experiment1 = ['B0005', 'B0006', 'B0007']
    df_x, df_y = get_exp_based_df(experiment1)
    train_x, test_x, train_y, test_y = train_test_split(df_x, df_y, test_size=0.2, random_state=0)

    # Normalize the data
    scaler = StandardScaler()
    train_x = scaler.fit_transform(train_x.reshape(-1, train_x.shape[-1])).reshape(train_x.shape)
    test_x = scaler.transform(test_x.reshape(-1, test_x.shape[-1])).reshape(test_x.shape)

    # Define the transformer model
    embed_dim = 32  # Embedding size for each token
    num_heads = 2  # Number of attention heads
    ff_dim = 32  # Hidden layer size in feed forward network inside transformer

    model = build_transformer_model(input_shape=train_x.shape[1:], embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim)
    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='mse')

    LEARNING_RATE = 0.0007
    REGULARIZATION = 0.0002
    EPOCHS = 10
    BATCH_SIZE = 64

    history = model.fit(train_x, train_y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(test_x, test_y))

    # Save the trained model
    model.save('rul_prediction_model_transformer.h5')
    loaded_model = keras.models.load_model('rul_prediction_model_transformer.h5', custom_objects={'TransformerBlock': TransformerBlock})

    # Plot training and validation loss
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.legend()
    plt.show()

    y_pred = model.predict(test_x)

    # Create a DataFrame to compare predictions and true values
    df_pred = pd.DataFrame(data={'pred': y_pred.flatten(), 'true': test_y})
    print(df_pred.head())

    # Plot the predictions and true values
    plt.figure()
    plt.plot(df_pred['pred'], label='Predicted')
    plt.plot(df_pred['true'], label='True')
    plt.legend()
    plt.show()
    new_mae = mean_absolute_error(new_df_pred['true'], new_df_pred['pred'])
    new_mse = mean_squared_error(new_df_pred['true'], new_df_pred['pred'])
    new_rmse = np.sqrt(new_mse)

    print(f"Evaluation Metrics for {new_experiment}:")
    print(f"Mean Absolute Error (MAE): {new_mae}")
    print(f"Mean Squared Error (MSE): {new_mse}")
    print(f"Root Mean Squared Error (RMSE): {new_rmse}")

experiment2 = ["B0025", "B0026", "B0027", "B0028"]
new_experiment = experiment2  # Replace with the new experiment name
new_test_x, new_test_y = get_exp_based_df(new_experiment)

# Normalize the new test data
max_len = train_x.shape[-1]  # Ensure the same length as training data
new_test_x_padded = np.zeros((new_test_x.shape[0], new_test_x.shape[1], max_len))
for i in range(new_test_x.shape[0]):
    for j in range(new_test_x.shape[1]):
        new_test_x_padded[i, j, :new_test_x.shape[2]] = new_test_x[i, j, :max_len]

new_test_x = scaler.transform(new_test_x_padded.reshape(-1, new_test_x_padded.shape[-1])).reshape(new_test_x_padded.shape)

# Make predictions on the new test data
new_y_pred = model.predict(new_test_x)

# Create a DataFrame to compare new predictions and true values
new_df_pred = pd.DataFrame(data={'pred': new_y_pred.flatten(), 'true': new_test_y})
print(new_df_pred.head())

# Plot the new predictions and true values
plt.figure()
plt.plot(new_df_pred['pred'], label='Predicted')
plt.plot(new_df_pred['true'], label='True')
plt.legend()
plt.show()
new_mae = mean_absolute_error(new_df_pred['true'], new_df_pred['pred'])
new_mse = mean_squared_error(new_df_pred['true'], new_df_pred['pred'])
new_rmse = np.sqrt(new_mse)

print(f"Evaluation Metrics for {new_experiment}:")
print(f"Mean Absolute Error (MAE): {new_mae}")
print(f"Mean Squared Error (MSE): {new_mse}")
print(f"Root Mean Squared Error (RMSE): {new_rmse}")

experiment3 = ["B0029", "B0030", "B0031", "B0032"]
experiment4 = ["B0033", "B0034", "B0036"]
experiment5 = ["B0038", "B0039", "B0040"]
experiment6 = ["B0041", "B0042", "B0043", "B0044"]
experiment7 = ["B0045", "B0046", "B0047", "B0048"]
experiment8 = ["B0049", "B0050", "B0051", "B0052"]
experiment9 = ["B0053", "B0054", "B0055", "B0056"]

y_pred = loaded_model.predict(test_x, batch_size=BATCH_SIZE)
y_pred_last_timestep = y_pred.flatten()
# from sklearn.metrics import mean_absolute_error, mean_squared_error

# Create a DataFrame to compare predictions and true values
df_pred = pd.DataFrame(data={'pred': y_pred_last_timestep, 'true': test_y})

# Print and visualize the first few predictions
print(df_pred.head())

plt.figure()
plt.plot(df_pred['pred'], label='Predicted')
plt.plot(df_pred['true'], label='True')
plt.legend()
plt.show()

# Calculate evaluation metrics for the test data
mae = mean_absolute_error(df_pred['true'], df_pred['pred'])
mse = mean_squared_error(df_pred['true'], df_pred['pred'])
rmse = np.sqrt(mse)

print(f"Evaluation Metrics for Test Data:")
print(f"Mean Absolute Error (MAE): {mae}")
print(f"Mean Squared Error (MSE): {mse}")
print(f"Root Mean Squared Error (RMSE): {rmse}")

# For loading and evaluating on new experiments (as per your previous code snippet)
for experiment in [experiment3, experiment4, experiment5, experiment6, experiment7, experiment8, experiment9]:
    # Load another experiment as test data
    new_experiment = experiment  # Replace with the new experiment name

    # Assuming new_y_pred is computed from loaded_model.predict(new_test_x_padded, batch_size=BATCH_SIZE)

    # Select the predictions for the last timestep (assuming single value prediction per sequence)
    new_y_pred_last_timestep = new_y_pred.flatten()

    # Create a DataFrame to compare predictions and true values
    new_df_pred = pd.DataFrame(data={'pred': new_y_pred_last_timestep, 'true': new_test_y})

    # Display the predictions
    print(new_df_pred.head())

    # Plot the predictions and true values
    plt.figure()
    plt.title(new_experiment)
    plt.plot(new_df_pred['pred'], label='Predicted')
    plt.plot(new_df_pred['true'], label='True')
    plt.legend()
    plt.show()

    # Calculate evaluation metrics for the new experiment data
    new_mae = mean_absolute_error(new_df_pred['true'], new_df_pred['pred'])
    new_mse = mean_squared_error(new_df_pred['true'], new_df_pred['pred'])
    new_rmse = np.sqrt(new_mse)

    print(f"Evaluation Metrics for {new_experiment}:")
    print(f"Mean Absolute Error (MAE): {new_mae}")
    print(f"Mean Squared Error (MSE): {new_mse}")
    print(f"Root Mean Squared Error (RMSE): {new_rmse}")

